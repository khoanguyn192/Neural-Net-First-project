{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN_from_scratch.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMKwpkoJVDP0jJKoQqXAeTh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PhatHuynhTranSon99/Neural-Networks-From-Scratch/blob/main/RNN_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RASS3efS9YkP"
      },
      "source": [
        "# About this notebook\n",
        "\n",
        "In this notebook, I will implement a simple neural network architecture: Recurrent Neural Network from scratch to perform a NLP task: sentiment analysis on small number of sentences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XlW_iNlBsCqL"
      },
      "source": [
        "# Library import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rAJJsFOKsEBf"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cS46S_ntsLAt"
      },
      "source": [
        "# Data generation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJjmmcyksMLM"
      },
      "source": [
        "# Dataset creation\n",
        "# Create a dataset for sentiment analysis\n",
        "train_data = {\n",
        "  'good': True,\n",
        "  'bad': False,\n",
        "  'happy': True,\n",
        "  'sad': False,\n",
        "  'not good': False,\n",
        "  'not bad': True,\n",
        "  'not happy': False,\n",
        "  'not sad': True,\n",
        "  'very good': True,\n",
        "  'very bad': False,\n",
        "  'very happy': True,\n",
        "  'very sad': False,\n",
        "  'i am happy': True,\n",
        "  'this is good': True,\n",
        "  'i am bad': False,\n",
        "  'this is bad': False,\n",
        "  'i am sad': False,\n",
        "  'this is sad': False,\n",
        "  'i am not happy': False,\n",
        "  'this is not good': False,\n",
        "  'i am not bad': True,\n",
        "  'this is not sad': True,\n",
        "  'i am very happy': True,\n",
        "  'this is very good': True,\n",
        "  'i am very bad': False,\n",
        "  'this is very sad': False,\n",
        "  'this is very happy': True,\n",
        "  'i am good not bad': True,\n",
        "  'this is good not bad': True,\n",
        "  'i am bad not good': False,\n",
        "  'i am good and happy': True,\n",
        "  'this is not good and not happy': False,\n",
        "  'i am not at all good': False,\n",
        "  'i am not at all bad': True,\n",
        "  'i am not at all happy': False,\n",
        "  'this is not at all sad': True,\n",
        "  'this is not at all happy': False,\n",
        "  'i am good right now': True,\n",
        "  'i am bad right now': False,\n",
        "  'this is bad right now': False,\n",
        "  'i am sad right now': False,\n",
        "  'i was good earlier': True,\n",
        "  'i was happy earlier': True,\n",
        "  'i was bad earlier': False,\n",
        "  'i was sad earlier': False,\n",
        "  'i am very bad right now': False,\n",
        "  'this is very good right now': True,\n",
        "  'this is very sad right now': False,\n",
        "  'this was bad earlier': False,\n",
        "  'this was very good earlier': True,\n",
        "  'this was very bad earlier': False,\n",
        "  'this was very happy earlier': True,\n",
        "  'this was very sad earlier': False,\n",
        "  'i was good and not bad earlier': True,\n",
        "  'i was not good and not happy earlier': False,\n",
        "  'i am not at all bad or sad right now': True,\n",
        "  'i am not at all good or happy right now': False,\n",
        "  'this was not happy and not good earlier': False,\n",
        "}\n",
        "\n",
        "test_data = {\n",
        "  'this is happy': True,\n",
        "  'i am good': True,\n",
        "  'this is not happy': False,\n",
        "  'i am not good': False,\n",
        "  'this is not bad': True,\n",
        "  'i am not sad': True,\n",
        "  'i am very good': True,\n",
        "  'this is very bad': False,\n",
        "  'i am very sad': False,\n",
        "  'this is bad not good': False,\n",
        "  'this is good and happy': True,\n",
        "  'i am not good and not happy': False,\n",
        "  'i am not at all sad': True,\n",
        "  'this is not at all good': False,\n",
        "  'this is not at all bad': True,\n",
        "  'this is good right now': True,\n",
        "  'this is sad right now': False,\n",
        "  'this is very bad right now': False,\n",
        "  'this was good earlier': True,\n",
        "  'i was not happy and not good earlier': False,\n",
        "}\n",
        "\n",
        "# Firstly, calculate the vocabulary size of training and test set\n",
        "word_to_index = {}\n",
        "current_index = 0\n",
        "\n",
        "for sentence in train_data:\n",
        "  # Split sentences to get words\n",
        "  words = sentence.split()\n",
        "\n",
        "  # Put into word to index\n",
        "  for word in words:\n",
        "    if word not in word_to_index:\n",
        "      word_to_index[word] = current_index\n",
        "      current_index += 1\n",
        "\n",
        "for sentence in test_data:\n",
        "  # Split sentences to get words\n",
        "  words = sentence.split()\n",
        "\n",
        "  # Put into word to index\n",
        "  for word in words:\n",
        "    if word not in word_to_index:\n",
        "      word_to_index[word] = current_index\n",
        "      current_index += 1\n",
        "\n",
        "# Function to one-hot-encode word\n",
        "def encode(word, word_to_index, vocab_size):\n",
        "  # Create a numpy array\n",
        "  encoding = np.zeros(vocab_size)\n",
        "\n",
        "  # Place 1 into position of word\n",
        "  encoding[word_to_index[word]] = 1\n",
        "\n",
        "  return encoding\n",
        "\n",
        "# Build the dataset from each sentences:\n",
        "def build_dataset(data, word_to_index, vocab_size):\n",
        "  # Initialize an array\n",
        "  dataset = []\n",
        "\n",
        "  # For each sentence\n",
        "  for sentence in data:\n",
        "    # Initialize current X and y\n",
        "    current_X = []\n",
        "    \n",
        "    # Get the label \n",
        "    label = data[sentence]\n",
        "    current_y = int(label)\n",
        "\n",
        "    # Split into words\n",
        "    words = sentence.split()\n",
        "\n",
        "    # One-hot-encode each word and put it in the database\n",
        "    for word in words:\n",
        "      current_X.append(encode(word, word_to_index, vocab_size))\n",
        "\n",
        "    # Then add X and Y into dataset\n",
        "    dataset.append((current_X, current_y))\n",
        "\n",
        "  return dataset\n",
        "\n",
        "# Create datasets for training and testing\n",
        "train_dataset = build_dataset(train_data, word_to_index, vocab_size=current_index)\n",
        "test_dataset = build_dataset(test_data, word_to_index, vocab_size=current_index)\n",
        "\n",
        "# Define dimensions\n",
        "input_size = len(word_to_index)\n",
        "hidden_size = 64"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7hP2kMUsokU"
      },
      "source": [
        "# Utility functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t84kmAQ_ssN_"
      },
      "source": [
        "def sigmoid(x):\n",
        "  return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def tanh(x):\n",
        "  return np.tanh(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UsVurUMGsO0Z"
      },
      "source": [
        "# Logistic regression layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9moKjysMsQri"
      },
      "source": [
        "class LogisticRegression:\n",
        "  def __init__(self, input_size, learning_rate):\n",
        "    self.input_size = input_size\n",
        "    self.learning_rate = learning_rate\n",
        "    self.u = np.random.randn(input_size)\n",
        "    self.b_y = 0\n",
        "\n",
        "  def forward(self, h):\n",
        "    self.h = h;\n",
        "    z = self.u.dot(h) + self.b_y\n",
        "    y_hat = sigmoid(z)\n",
        "    self.y_hat = y_hat\n",
        "    return y_hat\n",
        "\n",
        "  def backward(self, y):\n",
        "    dz = self.y_hat - y\n",
        "    du = dz * self.h\n",
        "    dh = dz * self.u\n",
        "    db_y = dz\n",
        "\n",
        "    self.u -= self.learning_rate * du\n",
        "    self.b_y -= self.learning_rate * db_y\n",
        "\n",
        "    return dh"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLHZnL5Ztwq4"
      },
      "source": [
        "# RNN layer\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oUH_-itHt0xo"
      },
      "source": [
        "class RNN:\n",
        "  def __init__(self, input_size, hidden_size, learning_rate):\n",
        "    self.input_size = input_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.learning_rate = learning_rate\n",
        "\n",
        "    self.W_hh = np.random.randn(hidden_size, hidden_size) / 1000\n",
        "    self.W_xh = np.random.randn(hidden_size, input_size) / 1000\n",
        "    self.b_h = np.zeros(hidden_size)\n",
        "\n",
        "  def forward(self, inputs):\n",
        "    self.x = inputs\n",
        "    self.h = []\n",
        "\n",
        "    h = np.zeros(self.hidden_size)\n",
        "    self.h.append(h)\n",
        "\n",
        "    for x in inputs:\n",
        "      h = tanh(self.W_hh @ h + self.W_xh @ x + self.b_h)\n",
        "      self.h.append(h)\n",
        "\n",
        "    return h\n",
        "\n",
        "  def backward(self, dh_T):\n",
        "    dW_hh = np.zeros_like(self.W_hh)\n",
        "    dW_xh = np.zeros_like(self.W_xh)\n",
        "    db_h = np.zeros_like(self.b_h)\n",
        "\n",
        "    input_length = len(self.x)\n",
        "\n",
        "    dh = dh_T\n",
        "    for t in reversed(range(input_length)):\n",
        "      da = dh * (1 - self.h[t + 1]**2)\n",
        "\n",
        "      dW_hh += np.outer(da, self.h[t])\n",
        "      dW_xh += np.outer(da, self.x[t])\n",
        "      db_h += da \n",
        "\n",
        "      dh = da @ self.W_hh\n",
        "\n",
        "    self.W_hh -= self.learning_rate * dW_hh\n",
        "    self.W_xh -= self.learning_rate * dW_xh\n",
        "    self.b_h -= self.learning_rate * db_h"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1DkVRA5xie9"
      },
      "source": [
        "# Putting things together"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TwrTY5xOxj6-",
        "outputId": "87f51cad-af46-49e2-dad2-6eb70902cc6b"
      },
      "source": [
        "rnn = RNN(input_size, 64, 0.01)\n",
        "log = LogisticRegression(64, 0.01)\n",
        "\n",
        "\n",
        "def train(rnn_layer, logistic_regression_layer, epochs = 2000):\n",
        "  for i in range(epochs):\n",
        "    loss = 0\n",
        "    train_correct = 0\n",
        "    test_correct = 0\n",
        "\n",
        "    # Loop through items in train dataset and train\n",
        "    for x, y in train_dataset:\n",
        "      h_T = rnn.forward(x)\n",
        "      y_hat = log.forward(h_T)\n",
        "\n",
        "      current_loss = - y * np.log(y_hat) - (1 - y) * np.log(1 - y_hat)\n",
        "      loss += current_loss\n",
        "\n",
        "      if (y == 0 and y_hat < 0.5) or (y == 1 and y_hat >= 0.5):\n",
        "        train_correct += 1\n",
        "\n",
        "      dh_T = log.backward(y)\n",
        "      rnn.backward(dh_T)\n",
        "\n",
        "    # Calculate test accuracy\n",
        "    for x, y in test_dataset:\n",
        "      h_T = rnn.forward(x)\n",
        "      y_hat = log.forward(h_T)\n",
        "\n",
        "      if (y == 0 and y_hat < 0.5) or (y == 1 and y_hat >= 0.5):\n",
        "        test_correct += 1\n",
        "\n",
        "    if i % 100 == 0 or i == 1999:\n",
        "      print(f\"Epochs: {i}\")\n",
        "      print(f\"Loss: {loss / len(train_dataset)}\")\n",
        "      print(f\"Train Accuracy: {train_correct / len(train_dataset)}\")\n",
        "      print(f\"Test Accuracy: {test_correct / len(test_dataset)}\")\n",
        "      print(\"--------------------\")\n",
        "\n",
        "train(rnn, log)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 0\n",
            "Loss: 0.8520817873878638\n",
            "Train Accuracy: 0.39655172413793105\n",
            "Test Accuracy: 0.5\n",
            "--------------------\n",
            "Epochs: 100\n",
            "Loss: 0.7482468796809268\n",
            "Train Accuracy: 0.43103448275862066\n",
            "Test Accuracy: 0.55\n",
            "--------------------\n",
            "Epochs: 200\n",
            "Loss: 0.5228762863833267\n",
            "Train Accuracy: 0.6551724137931034\n",
            "Test Accuracy: 0.65\n",
            "--------------------\n",
            "Epochs: 300\n",
            "Loss: 0.42500039945212914\n",
            "Train Accuracy: 0.7241379310344828\n",
            "Test Accuracy: 0.7\n",
            "--------------------\n",
            "Epochs: 400\n",
            "Loss: 0.03182226590207598\n",
            "Train Accuracy: 1.0\n",
            "Test Accuracy: 1.0\n",
            "--------------------\n",
            "Epochs: 500\n",
            "Loss: 0.010655381822194141\n",
            "Train Accuracy: 1.0\n",
            "Test Accuracy: 1.0\n",
            "--------------------\n",
            "Epochs: 600\n",
            "Loss: 0.006112877237748109\n",
            "Train Accuracy: 1.0\n",
            "Test Accuracy: 1.0\n",
            "--------------------\n",
            "Epochs: 700\n",
            "Loss: 0.004207682869785306\n",
            "Train Accuracy: 1.0\n",
            "Test Accuracy: 1.0\n",
            "--------------------\n",
            "Epochs: 800\n",
            "Loss: 0.0031769381321984772\n",
            "Train Accuracy: 1.0\n",
            "Test Accuracy: 1.0\n",
            "--------------------\n",
            "Epochs: 900\n",
            "Loss: 0.0025367817047869824\n",
            "Train Accuracy: 1.0\n",
            "Test Accuracy: 1.0\n",
            "--------------------\n",
            "Epochs: 1000\n",
            "Loss: 0.0021029983425976412\n",
            "Train Accuracy: 1.0\n",
            "Test Accuracy: 1.0\n",
            "--------------------\n",
            "Epochs: 1100\n",
            "Loss: 0.0017908269634933944\n",
            "Train Accuracy: 1.0\n",
            "Test Accuracy: 1.0\n",
            "--------------------\n",
            "Epochs: 1200\n",
            "Loss: 0.001556051024383738\n",
            "Train Accuracy: 1.0\n",
            "Test Accuracy: 1.0\n",
            "--------------------\n",
            "Epochs: 1300\n",
            "Loss: 0.0013734346826345207\n",
            "Train Accuracy: 1.0\n",
            "Test Accuracy: 1.0\n",
            "--------------------\n",
            "Epochs: 1400\n",
            "Loss: 0.0012275648944865827\n",
            "Train Accuracy: 1.0\n",
            "Test Accuracy: 1.0\n",
            "--------------------\n",
            "Epochs: 1500\n",
            "Loss: 0.0011085153539858834\n",
            "Train Accuracy: 1.0\n",
            "Test Accuracy: 1.0\n",
            "--------------------\n",
            "Epochs: 1600\n",
            "Loss: 0.0010096147240925039\n",
            "Train Accuracy: 1.0\n",
            "Test Accuracy: 1.0\n",
            "--------------------\n",
            "Epochs: 1700\n",
            "Loss: 0.0009262192039562017\n",
            "Train Accuracy: 1.0\n",
            "Test Accuracy: 1.0\n",
            "--------------------\n",
            "Epochs: 1800\n",
            "Loss: 0.000854999982452847\n",
            "Train Accuracy: 1.0\n",
            "Test Accuracy: 1.0\n",
            "--------------------\n",
            "Epochs: 1900\n",
            "Loss: 0.0007935105794034635\n",
            "Train Accuracy: 1.0\n",
            "Test Accuracy: 1.0\n",
            "--------------------\n",
            "Epochs: 1999\n",
            "Loss: 0.0007404157644258689\n",
            "Train Accuracy: 1.0\n",
            "Test Accuracy: 1.0\n",
            "--------------------\n"
          ]
        }
      ]
    }
  ]
}