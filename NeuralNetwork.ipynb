{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NeuralNetwork.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNnTh52iUYnxOPu5mU/oyjh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PhatHuynhTranSon99/Neural-Network-From-Scratch/blob/main/NeuralNetwork.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ajHDcYlGKmS7"
      },
      "source": [
        "# Neural Network from stratch\n",
        "\n",
        "## About\n",
        "Neural Networks are often blackboxes some abitrary inputs to your desired output (whether it's a continuous value or a categorical value). In this colab, I hope to uncover to essential details on how to implement common layers in a standard feed-forward neural network.\n",
        "\n",
        "## Task\n",
        "The task we have at hand is that of classification, where the neural network is required to accurately predict the class of a 2-dimensional datapoints. The dataset used for this example will be generated from 2-spiral dataset. To do this, we make use of several layers, namely:\n",
        "\n",
        "\n",
        "*   Fully-connected layer\n",
        "*   Sigmoid layer\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPFNESf-MDdF"
      },
      "source": [
        "## Library import\n",
        "\n",
        "For this example, we will use only numpy to generate the dataset and train the neural network. We will also use matplotlib to plot the values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ekRcPzsJMZ0s"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5uZeMzSMtr1"
      },
      "source": [
        "## Data Generation\n",
        "\n",
        "This little script generates a random 2-d spiral dataset using numpy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Rp5Ef-0M0hM"
      },
      "source": [
        "N = 400     #Number of datapoints\n",
        "pi = np.pi  # 3.14\n",
        "theta = np.sqrt(np.random.rand(N))*2*pi\n",
        "\n",
        "# Generate the positive points\n",
        "r_a = 2*theta + pi\n",
        "data_a = np.array([np.cos(theta)*r_a, np.sin(theta)*r_a]).T\n",
        "positive_points = data_a + np.random.randn(N,2)\n",
        "\n",
        "# Generate the negative points\n",
        "r_b = -2*theta - pi\n",
        "data_b = np.array([np.cos(theta)*r_b, np.sin(theta)*r_b]).T\n",
        "negative_points = data_b + np.random.randn(N,2)\n",
        "\n",
        "# Combine x and y (the labels: 0 or 1)\n",
        "negative_data = np.append(negative_points, np.zeros((N,1)), axis=1)\n",
        "positive_data = np.append(positive_points, np.ones((N,1)), axis=1)\n",
        "\n",
        "# Combine positive points and negative points to create a dataset\n",
        "data = np.append(negative_data, positive_data, axis=0)\n",
        "np.random.shuffle(data) # Shuffle the data in abitrary order"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VwqMHs_aN8ZI"
      },
      "source": [
        "The dataset generated's plot is as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "4YF2RKLXOCG1",
        "outputId": "44b2bb1e-3b6e-4308-e26a-56966b7f4938"
      },
      "source": [
        "plt.scatter(positive_points[:,0], positive_points[:,1])\n",
        "plt.scatter(negative_points[:,0], negative_points[:,1])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FB_nr4QmOWrd"
      },
      "source": [
        "## Defining the layers\n",
        "\n",
        "In this section, we define two layers (Fully connected and sigmoid) with forward and backward function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYUP2kLG_EYv"
      },
      "source": [
        "### Full-connected layer\n",
        "\n",
        "This layer consists of a multiplication of the input with the weight matrix W and addition with the bias b"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uUDTk4CQ_GuK"
      },
      "source": [
        "class FullyConnected:\n",
        "  def __init__(self, m, n):\n",
        "    \"\"\"\n",
        "    Params:\n",
        "      - m: the number of rows of weight matrix W\n",
        "      - n: the number of columns of weight matrix W\n",
        "    Puspose:\n",
        "      - To initialize the weight matrix W (dimension m x n) and \n",
        "        a vector bias b\n",
        "      - Weight matrix W is initialized with random uniform weights (to avoid the problem of symmetry)\n",
        "      - Bias vector b is initialize with zeros\n",
        "    \"\"\"\n",
        "    self.W = np.random.rand(m, n)\n",
        "    self.b = np.zeros(m)\n",
        "\n",
        "  def forward(self, x):\n",
        "    \"\"\"\n",
        "    Params:\n",
        "      - x: input of dimension k x m\n",
        "    Purpose:\n",
        "      - Perform a forward pass (output = Wx + b)\n",
        "      - Cache the input value (x) for backward step\n",
        "    \"\"\"\n",
        "    self.x = x\n",
        "    return self.W @ x + self.b\n",
        "\n",
        "  def backward(self, dL, learning_rate = 0.01):\n",
        "    \"\"\"\n",
        "    Params:\n",
        "      - dL: propagated loss from deeper layer (dL/dz with z: the value of forward pass)\n",
        "    Purpose:\n",
        "      - Find the loss gradient with respect to W, b and x i.e to produce dL/dW and dL/db and dX\n",
        "      - Update W and b for gradient descent\n",
        "      - Define the upstream gradient dX (for shallower layers)\n",
        "    \"\"\"\n",
        "\n",
        "    # Produce the gradient\n",
        "    dW = np.outer(dL, self.x)\n",
        "    dX = dL @ self.W\n",
        "    db = dL\n",
        "\n",
        "    # Perform gradient descent\n",
        "    self.W -= learning_rate * dW\n",
        "    self.b -= learning_rate * db\n",
        "\n",
        "    # Returns the upstream gradient dX\n",
        "    return dX"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_CaHT47FQnn"
      },
      "source": [
        "### Test fully-connected layer\n",
        "\n",
        "In this part, we check if the forward and background pass of the implementation produce vectors and matrices of correct shapes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HDQlYFxZFchj"
      },
      "source": [
        "# Define an input of size 3 x 2\n",
        "x = np.random.rand(3)\n",
        "\n",
        "# Define a fully connecte layer of size (2 x 4) (m = 2 and n = 4)\n",
        "fully_connected = FullyConnected(4, 3)\n",
        "\n",
        "# Get output for forward pass and backward pass (with random loss graidne tfor backward pass)\n",
        "forward_output = fully_connected.forward(x)\n",
        "backward_output = fully_connected.backward( np.random.rand(4) )\n",
        "\n",
        "# Check the dimensions\n",
        "assert forward_output.shape == (4,)\n",
        "assert backward_output.shape == (3,)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4GKnTwrSHU0B"
      },
      "source": [
        "As one may observe, the forwad and background produce vectors and matrices of correct shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQur-CSrHfHy"
      },
      "source": [
        "### Sigmoid layer\n",
        "\n",
        "This section will define the Sigmoid layer, together with its forward and backward methods"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pnbtinr4HmoP"
      },
      "source": [
        "class Sigmoid:\n",
        "  def sigmoid(self, x):\n",
        "    \"\"\"\n",
        "    Params:\n",
        "      - x: Input vector (with dimension n x 1)\n",
        "    Purpose:\n",
        "      - Calculate the sigmoid for each entry of the input vector x\n",
        "    \"\"\"\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "  def forward(self, x):\n",
        "    \"\"\"\n",
        "    Params:\n",
        "      - x: Input vector (with dimension n x 1)\n",
        "    Purpose:\n",
        "      - Calculate the forward step of the input vector x i.e produce a vector of size (n x 1) also\n",
        "      - Cache the value of input vector x (for the backward step)\n",
        "    \"\"\"\n",
        "    self.x = x\n",
        "    return self.sigmoid(x)\n",
        "\n",
        "  def backward(self, dL):\n",
        "    \"\"\"\n",
        "    Params:\n",
        "      - dL: Propagated loss from deeper layers\n",
        "    Purpose:\n",
        "      - Calculate the backward step of the input vector x i.e produce a vector of size (n x 1) also\n",
        "      - Return the gradient as upstream gradient for shallower layers\n",
        "      - Don't perform any kind of gradient descent (this layer doesn't have any parameters)\n",
        "    \"\"\"\n",
        "    df = self.sigmoid(self.x) * ( 1 - self.sigmoid(self.x) )\n",
        "    dX = dL * df # dL/dx = dL/df x df/dX\n",
        "    return dX"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6Gn4XFFKIUc"
      },
      "source": [
        "### Test sigmoid layer\n",
        "\n",
        "In this part, we check if the forward and background pass of the implementation produce vectors and matrices of correct shapes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gm9Sa5cnKIpg"
      },
      "source": [
        "# Define an input of size 3 x 2\n",
        "x = np.random.rand(3)\n",
        "\n",
        "# Define a fully connecte layer of size (2 x 4) (m = 2 and n = 4)\n",
        "sigmoid = Sigmoid()\n",
        "\n",
        "# Get output for forward pass and backward pass (with random loss graidne tfor backward pass)\n",
        "forward_output = sigmoid.forward(x)\n",
        "backward_output = sigmoid.backward( np.random.rand(3) )\n",
        "\n",
        "# Check the dimensions\n",
        "assert forward_output.shape == (3,)\n",
        "assert backward_output.shape == (3,)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RU-689vDLvAj"
      },
      "source": [
        "## Define the loss layer\n",
        "\n",
        "This section will define a class for calculate cross entropy loss for a single prediction y_hat and y. It will have 2 method, one for calculating loss and another for calculating the gradient of the loss with respect to the prediction y_hat"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71NPEc2rcKrJ"
      },
      "source": [
        "class CrossEntropy:\n",
        "  def forward(self, y, y_hat):\n",
        "    \"\"\"\n",
        "    Params:\n",
        "      - y: The true value of the label (for one example (x_i, y_i))\n",
        "      - y_hat:  The predicted value of the label (for one example (x_i, y_i)).\n",
        "        This is produced by our learning algorithm\n",
        "    Purpose:\n",
        "      - To cache the value of y and y_hat (for back propagation step)\n",
        "      - To calculate the cross entropy loss of one learning example (x_i, y_i)\n",
        "      - To return the cross entropy loss\n",
        "    \"\"\"\n",
        "    self.y = y\n",
        "    self.y_hat = y_hat\n",
        "    return - y * np.log(y_hat) - (1 - y) * np.log(1 - y_hat)\n",
        "\n",
        "  def backward(self):\n",
        "    \"\"\"\n",
        "    Params:\n",
        "      - None\n",
        "    Purpose:\n",
        "      - To calculate the cross entropy loss's gradient of one learning example \n",
        "        (x_i, y_i) with respect to the prediction (y_hat) dL / dy_hat\n",
        "      - To return the cross entropy loss's gradient\n",
        "    \"\"\"\n",
        "    y = self.y\n",
        "    y_hat = self.y_hat\n",
        "\n",
        "    return (y_hat - y) / (y_hat * (1 - y_hat))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WWvnWvtd956"
      },
      "source": [
        "## Test the loss layer\n",
        "\n",
        "This section will test the cross enptropy loss layer's result (by checking the dimesions)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RwPwh3bsfWgv"
      },
      "source": [
        "# Define one example true and predicted label\n",
        "y = 1\n",
        "y_predicted = 0.8\n",
        "\n",
        "# Create cross entropy layer\n",
        "cross_entropy = CrossEntropy()\n",
        "\n",
        "# Calculate the forward and backward pass\n",
        "forward_result = cross_entropy.forward(y, y_hat = y_predicted)\n",
        "backward_result = cross_entropy.backward()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1b3sOD8RKvUE"
      },
      "source": [
        "## Define Neural Network implementation\n",
        "\n",
        "In this section, we define the basics of Neural Network class. This implementation will have no layers initially and it's up the the users to add in the layers. Its only reponsibility is calling the classes' forward and backward methods to perform training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qLPzIHfALV_m"
      },
      "source": [
        "class NeuralNetwork:\n",
        "  def __init__(self, loss, learning_rate = 0.001):\n",
        "    \"\"\"\n",
        "    Params:\n",
        "      - loss: The loss layer used for calculating the loss function for one\n",
        "        training example\n",
        "      - learning_rate: The learning rate (alpha) of the neural network\n",
        "        which is used for gradient descent\n",
        "    Purpose:\n",
        "      - To set the loss for the neural network\n",
        "      - To initialize the layers array of the neural network\n",
        "      - To set the learning rate of the neural network\n",
        "    \"\"\"\n",
        "    self.learning_rate = learning_rate\n",
        "    self.loss = loss\n",
        "    self.layers = []\n",
        "\n",
        "  def add(self, layer):\n",
        "    \"\"\"\n",
        "    Params:\n",
        "      - layer: A layer of the neural network, this can be any of the \n",
        "        FullyConnected or Sigmoid layer above. You can extend this to more\n",
        "        layers by defining your own and adding forward and backward \n",
        "        methods\n",
        "    Purpose:\n",
        "      - To add a layer to the neural network. This layer will be saved in the\n",
        "        self.layers property\n",
        "    \"\"\"\n",
        "    self.layers.append(layer)\n",
        "\n",
        "  def forward(self, x, y):\n",
        "    \"\"\"\n",
        "    Params:\n",
        "      - x: The input vector to neural network (vector of dimension p)\n",
        "    Purpose:\n",
        "      - To calculate the loss of the network (for one example) \n",
        "        using each layer's own propagation method and the loss function\n",
        "      - To return the loss to the caller\n",
        "    \"\"\"\n",
        "\n",
        "    # Calculate the result of the last layer\n",
        "    result = x\n",
        "    for layer in self.layers:\n",
        "      result = layer.forward(result)\n",
        "\n",
        "    # Calculate the loss of the prediction\n",
        "    loss = self.loss.forward(y, y_hat = result)\n",
        "\n",
        "    return result, loss\n",
        "\n",
        "  def backward(self):\n",
        "    \"\"\"\n",
        "    Params:\n",
        "      - None\n",
        "    Purpose:\n",
        "      - To perform an iteration of backpropagation of the input by calling each\n",
        "        layer's backward method\n",
        "      - To return the gradient with respect to the input x\n",
        "    \"\"\"\n",
        "    \n",
        "    # First initialize the initial loss gradient \n",
        "    # (w.r.t last layer of the network)\n",
        "    dL = self.loss.backward()\n",
        "\n",
        "    # Then propagate the loss to shallower layers\n",
        "    for layer in self.layers[::-1]:\n",
        "      dL = layer.backward(dL)\n",
        "\n",
        "    # Return the gradient w.r.t to the input (callers can optionally use this)\n",
        "    # result\n",
        "    return dL\n",
        "  \n",
        "  def train(self, training_data, max_iterations=10):\n",
        "    \"\"\"\n",
        "    Params:\n",
        "      - training_data: the training dataset containing all training examples\n",
        "        x_i and y_i\n",
        "      - max_iterations: the number of iterations to go through the entire\n",
        "        dataset and training\n",
        "    Purpose:\n",
        "      - To train the neural network by using the forward and backward methods\n",
        "      - To calculate the average loss of the dataset after each iteration\n",
        "      - To display the training loss after 10 iterations\n",
        "    \"\"\"\n",
        "\n",
        "    # Get the dimensions of training data (i.e the number of training dataset)\n",
        "    m = training_data.shape[0]\n",
        "\n",
        "    # Train the network max_iterations times\n",
        "    for iteration in range(max_iterations):\n",
        "      # First shuffle the training example so that the training is truly \n",
        "      # stochastic\n",
        "      np.random.shuffle\n",
        "\n",
        "      # We also save a cummulated loss to later calculate the average loss of\n",
        "      # the current iteration\n",
        "      cummulated_loss = 0\n",
        "\n",
        "      # For each iteration, we go through every datapoint (single x_i and y_i)\n",
        "      # to train\n",
        "      for training_example in training_data:\n",
        "        # We need to extract the x_i and y_i from the training_example as they\n",
        "        # were coupled together (y is the final element in the training_example)\n",
        "        # array\n",
        "        x_i = training_example[:-1]\n",
        "        y_i = training_example[-1]\n",
        "\n",
        "        # Run the forward propagation method on this example to calculate the \n",
        "        # loss. We use np.squezze and unwrapping syntax to make the loss a \n",
        "        # scalar\n",
        "        training_example_loss = self.forward(x_i, y_i)[1][0]\n",
        "\n",
        "        # Add to cummulated loss \n",
        "        cummulated_loss += training_example_loss\n",
        "\n",
        "        # Now we run backprop on this example to actually training the network\n",
        "        self.backward()\n",
        "\n",
        "      # After all examples have been looked at, finally the average loss is\n",
        "      # calculated and display to the caller after each 100 iterations (m is the \n",
        "      # number of training \n",
        "      # example)\n",
        "      if iteration % 100 == 0:\n",
        "        average_loss = cummulated_loss / m\n",
        "        print(f\"Average loss for iteration { iteration } is {average_loss}\");\n",
        "    \n",
        "  def predict(self, x):\n",
        "    \"\"\"\n",
        "    Params: \n",
        "     - x: the feature (a matrix of size m x n - #examples x #dimensions)\n",
        "    Purpose:\n",
        "     - To generate the prediction based on the input matrix x\n",
        "     - To return the vector y_hat (vector of size (m,))\n",
        "    \"\"\"\n",
        "\n",
        "    # First we run self.forward for each example x_i and get the prediction\n",
        "    # (first element of the tuple result is the prediction). This is another \n",
        "    # array so we just need to grap the first element by using[0]. Then use list\n",
        "    # comprehension to put the prediction into an array and finally use np.array\n",
        "    # to convert the array into a numpy vector\n",
        "    return np.array([ self.forward(x_i, 0)[0][0] for x_i in x])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "niITl4ssis_0"
      },
      "source": [
        "### Test the Neural Network implementation\n",
        "\n",
        "This section will test the forward and backward method of the Neural Network (with all the layers and loss defined above). A random training example will be used for this purpose"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-XpgAfAjChT"
      },
      "source": [
        "# Define a random training example (x and y)\n",
        "x = np.random.rand(2)\n",
        "y = 1\n",
        "\n",
        "# Create a neural network (with one hidden layer)\n",
        "# and cross entropy loss with random learning rate\n",
        "network = NeuralNetwork(loss = CrossEntropy(), learning_rate=0.001)\n",
        "\n",
        "# Add hidden layer\n",
        "network.add(FullyConnected(3, 2))\n",
        "network.add(Sigmoid())\n",
        "\n",
        "# Add output layer\n",
        "network.add(FullyConnected(1, 3))\n",
        "network.add(Sigmoid())\n",
        "\n",
        "# Test the forward and backward result\n",
        "forward_output = network.forward(x, y)\n",
        "backward_output = network.backward()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UK5rTGFYU20z"
      },
      "source": [
        "## Test the network with the spiral dataset\n",
        "\n",
        "Here we finally get to test the train method implemeted in the NeuralNetwork class on the spiral dataset created above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wti28BlYVEzo",
        "outputId": "6139f6d1-f0b8-4840-bed4-702986f8e0c0"
      },
      "source": [
        "# Create neural network with cross entropy loss and random learning rate\n",
        "neural_network = NeuralNetwork(loss = CrossEntropy(), learning_rate=0.001)\n",
        "\n",
        "# Add hidden layer\n",
        "neural_network.add(FullyConnected(3, 2))\n",
        "neural_network.add(Sigmoid())\n",
        "\n",
        "# Add output layer\n",
        "neural_network.add(FullyConnected(1, 3))\n",
        "neural_network.add(Sigmoid())\n",
        "\n",
        "# Train the network by call train method and providing training data\n",
        "neural_network.train(data, max_iterations=1000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average loss for iteration 0 is 0.6654598394261038\n",
            "Average loss for iteration 100 is 0.24886155727137574\n",
            "Average loss for iteration 200 is 0.20229297292268086\n",
            "Average loss for iteration 300 is 0.19550567025863136\n",
            "Average loss for iteration 400 is 0.19235686164884036\n",
            "Average loss for iteration 500 is 0.19078045331299034\n",
            "Average loss for iteration 600 is 0.1897202824269937\n",
            "Average loss for iteration 700 is 0.1888497820604258\n",
            "Average loss for iteration 800 is 0.1880385601388058\n",
            "Average loss for iteration 900 is 0.18734461683741693\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8601BJvuarTZ"
      },
      "source": [
        "Now we can check the accuracy rate of the neural network on the training set by calling neural_network.predict() and compare it to the true result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l8-TbJbOa4pU",
        "outputId": "3f921245-9387-4927-eaa8-fd775cb20ee8"
      },
      "source": [
        "# Split data into x (features) and y (label)\n",
        "x = data[:, :-1]\n",
        "y = data[:, -1]\n",
        "\n",
        "# Now use the predict method of the neural network to generate the prediction\n",
        "probabilities = neural_network.predict(x)\n",
        "\n",
        "# Convert to y_hat (by comparing to 0.5)\n",
        "y_hat = probabilities >= 0.5\n",
        "\n",
        "# Calculate the number of correct examples (by comparing y and y_hat and\n",
        "# summing)\n",
        "correct_examples = np.sum(y_hat == y)\n",
        "\n",
        "# Calculate the accuracy (by dividing witht the total number of training \n",
        "# examples)\n",
        "total_examples = y.shape[0]\n",
        "prediction_accuracy = correct_examples / total_examples\n",
        "print(f\"The accuracy for the training set is { prediction_accuracy }\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The accuracy for the training set is 0.93\n"
          ]
        }
      ]
    }
  ]
}