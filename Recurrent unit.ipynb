{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Gated_Recurrent_Unit_from_scratch.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNRMEnQ97f/YXjtuCn2ykrs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PhatHuynhTranSon99/Neural-Networks-From-Scratch/blob/main/Gated_Recurrent_Unit_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Om3UId5i7-5"
      },
      "source": [
        "# Gated Recurrent Unit from scratch (using only Numpy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ul1vM8rQjBt7"
      },
      "source": [
        "## Library import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5WNhvZ9LjFTF"
      },
      "source": [
        "import numpy as np\n",
        "import random"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_M7tZ8zsuVaW"
      },
      "source": [
        "## Utility functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ROxuuMRduXux"
      },
      "source": [
        "def softmax(x):\n",
        "  return np.exp(x) / np.sum(np.exp(x))\n",
        "\n",
        "def tanh(x):\n",
        "  return np.tanh(x)\n",
        "\n",
        "def sigmoid(x):\n",
        "  return 1 / (1 + np.exp(-x))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S2TMogU_jPHx"
      },
      "source": [
        "## Create sentiment analysis dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nnTSE8AxjRXY"
      },
      "source": [
        "# Create a dataset for sentiment analysis\n",
        "train_data = {\n",
        "  'good': True,\n",
        "  'bad': False,\n",
        "  'happy': True,\n",
        "  'sad': False,\n",
        "  'not good': False,\n",
        "  'not bad': True,\n",
        "  'not happy': False,\n",
        "  'not sad': True,\n",
        "  'very good': True,\n",
        "  'very bad': False,\n",
        "  'very happy': True,\n",
        "  'very sad': False,\n",
        "  'i am happy': True,\n",
        "  'this is good': True,\n",
        "  'i am bad': False,\n",
        "  'this is bad': False,\n",
        "  'i am sad': False,\n",
        "  'this is sad': False,\n",
        "  'i am not happy': False,\n",
        "  'this is not good': False,\n",
        "  'i am not bad': True,\n",
        "  'this is not sad': True,\n",
        "  'i am very happy': True,\n",
        "  'this is very good': True,\n",
        "  'i am very bad': False,\n",
        "  'this is very sad': False,\n",
        "  'this is very happy': True,\n",
        "  'i am good not bad': True,\n",
        "  'this is good not bad': True,\n",
        "  'i am bad not good': False,\n",
        "  'i am good and happy': True,\n",
        "  'this is not good and not happy': False,\n",
        "  'i am not at all good': False,\n",
        "  'i am not at all bad': True,\n",
        "  'i am not at all happy': False,\n",
        "  'this is not at all sad': True,\n",
        "  'this is not at all happy': False,\n",
        "  'i am good right now': True,\n",
        "  'i am bad right now': False,\n",
        "  'this is bad right now': False,\n",
        "  'i am sad right now': False,\n",
        "  'i was good earlier': True,\n",
        "  'i was happy earlier': True,\n",
        "  'i was bad earlier': False,\n",
        "  'i was sad earlier': False,\n",
        "  'i am very bad right now': False,\n",
        "  'this is very good right now': True,\n",
        "  'this is very sad right now': False,\n",
        "  'this was bad earlier': False,\n",
        "  'this was very good earlier': True,\n",
        "  'this was very bad earlier': False,\n",
        "  'this was very happy earlier': True,\n",
        "  'this was very sad earlier': False,\n",
        "  'i was good and not bad earlier': True,\n",
        "  'i was not good and not happy earlier': False,\n",
        "  'i am not at all bad or sad right now': True,\n",
        "  'i am not at all good or happy right now': False,\n",
        "  'this was not happy and not good earlier': False,\n",
        "}\n",
        "\n",
        "test_data = {\n",
        "  'this is happy': True,\n",
        "  'i am good': True,\n",
        "  'this is not happy': False,\n",
        "  'i am not good': False,\n",
        "  'this is not bad': True,\n",
        "  'i am not sad': True,\n",
        "  'i am very good': True,\n",
        "  'this is very bad': False,\n",
        "  'i am very sad': False,\n",
        "  'this is bad not good': False,\n",
        "  'this is good and happy': True,\n",
        "  'i am not good and not happy': False,\n",
        "  'i am not at all sad': True,\n",
        "  'this is not at all good': False,\n",
        "  'this is not at all bad': True,\n",
        "  'this is good right now': True,\n",
        "  'this is sad right now': False,\n",
        "  'this is very bad right now': False,\n",
        "  'this was good earlier': True,\n",
        "  'i was not happy and not good earlier': False,\n",
        "}"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CvLxZdeVjZG1"
      },
      "source": [
        "# Firstly, calculate the vocabulary size of training and test set\n",
        "word_to_index = {}\n",
        "current_index = 0\n",
        "\n",
        "for sentence in train_data:\n",
        "  # Split sentences to get words\n",
        "  words = sentence.split()\n",
        "\n",
        "  # Put into word to index\n",
        "  for word in words:\n",
        "    if word not in word_to_index:\n",
        "      word_to_index[word] = current_index\n",
        "      current_index += 1\n",
        "\n",
        "for sentence in test_data:\n",
        "  # Split sentences to get words\n",
        "  words = sentence.split()\n",
        "\n",
        "  # Put into word to index\n",
        "  for word in words:\n",
        "    if word not in word_to_index:\n",
        "      word_to_index[word] = current_index\n",
        "      current_index += 1\n",
        "\n",
        "# Function to one-hot-encode word\n",
        "def encode(word, word_to_index, vocab_size):\n",
        "  # Create a numpy array\n",
        "  encoding = np.zeros(vocab_size)\n",
        "\n",
        "  # Place 1 into position of word\n",
        "  encoding[word_to_index[word]] = 1\n",
        "\n",
        "  return encoding\n",
        "\n",
        "# Build the dataset from each sentences:\n",
        "def build_dataset(data, word_to_index, vocab_size):\n",
        "  # Initialize an array\n",
        "  dataset = []\n",
        "\n",
        "  # For each sentence\n",
        "  for sentence in data:\n",
        "    # Initialize current X and y\n",
        "    current_X = []\n",
        "    \n",
        "    # Get the label \n",
        "    label = data[sentence]\n",
        "    current_y = int(label)\n",
        "\n",
        "    # Split into words\n",
        "    words = sentence.split()\n",
        "\n",
        "    # One-hot-encode each word and put it in the database\n",
        "    for word in words:\n",
        "      current_X.append(encode(word, word_to_index, vocab_size))\n",
        "\n",
        "    # Then add X and Y into dataset\n",
        "    dataset.append((current_X, current_y))\n",
        "\n",
        "  return dataset\n",
        "\n",
        "# Create datasets for training and testing\n",
        "train_dataset = build_dataset(train_data, word_to_index, vocab_size=current_index)\n",
        "test_dataset = build_dataset(test_data, word_to_index, vocab_size=current_index)\n",
        "\n",
        "# Define dimensions\n",
        "input_size = len(word_to_index)\n",
        "hidden_size = 64"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VKjJrbLJjcNl",
        "outputId": "4397d458-16bb-4df9-aca3-b4444593f0bb"
      },
      "source": [
        "# Check the training dataset\n",
        "input, label = train_dataset[20]\n",
        "# Display\n",
        "print(\"X: \", input)\n",
        "print(\"Y: \", label)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X:  [array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0.]), array([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0.]), array([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0.]), array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0.])]\n",
            "Y:  1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ePgbaOikNri"
      },
      "source": [
        "## Define layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vop9cRjAkQgh"
      },
      "source": [
        "### Softmax layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yI-LQTKkkqX4"
      },
      "source": [
        "class Softmax:\n",
        "  def __init__(self, input_size, output_size):\n",
        "    # Cache the size\n",
        "    self.input_size = input_size\n",
        "    self.output_size = output_size\n",
        "\n",
        "    # Create the parameters for learning\n",
        "    self.W = np.random.randn(self.output_size, self.input_size) / 1000\n",
        "    self.b = np.zeros((self.output_size,))\n",
        "\n",
        "  def forward(self, x, y):\n",
        "    # Cache the input x and output yy\n",
        "    self.x = x\n",
        "    self.y = y\n",
        "\n",
        "    # Perform softmax calculation\n",
        "    # And cache the result\n",
        "    z = self.W @ x + self.b\n",
        "    y_hat = softmax(z)\n",
        "    self.y_hat = y_hat\n",
        "\n",
        "    # Calculate the loss\n",
        "    loss = - np.log(y_hat[y])\n",
        "\n",
        "    # Return both the loss and z\n",
        "    return y_hat, loss\n",
        "\n",
        "  def backward(self, learning_rate=0.01):\n",
        "    # Calculate dLoss/dz\n",
        "    dz = - self.y_hat\n",
        "    dz[self.y] -= 1\n",
        "\n",
        "    # Calculate dLoss/dW and dLoss/dz\n",
        "    dW = np.outer(dz, self.x)\n",
        "    db = dz\n",
        "\n",
        "    # Perform gradient descent\n",
        "    self.W -= learning_rate * dW\n",
        "    self.b -= learning_rate * db\n",
        "\n",
        "    # Calculate the loss w.r.t to input\n",
        "    dx = dz @ self.W\n",
        "\n",
        "    return dx"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WO7nIpszZmH"
      },
      "source": [
        "class LogisticRegression:\n",
        "  def __init__(self, input_size, learning_rate):\n",
        "    self.input_size = input_size\n",
        "    self.learning_rate = learning_rate\n",
        "    self.u = np.random.randn(input_size)\n",
        "    self.b_y = 0\n",
        "\n",
        "  def forward(self, h):\n",
        "    self.h = h;\n",
        "    z = self.u.dot(h) + self.b_y\n",
        "    y_hat = sigmoid(z)\n",
        "    self.y_hat = y_hat\n",
        "    return y_hat\n",
        "\n",
        "  def backward(self, y):\n",
        "    dz = self.y_hat - y\n",
        "    du = dz * self.h\n",
        "    dh = dz * self.u\n",
        "    db_y = dz\n",
        "\n",
        "    self.u -= self.learning_rate * du\n",
        "    self.b_y -= self.learning_rate * db_y\n",
        "\n",
        "    return dh"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELedIYltskG_"
      },
      "source": [
        "# Test softmax layer\n",
        "x = np.random.randn(5)\n",
        "softmax_layer = Softmax(5, 3)\n",
        "\n",
        "# Forward pass\n",
        "y_hat, loss = softmax_layer.forward(x, 2)\n",
        "\n",
        "# Check if y_hat sums to 1\n",
        "# and match the dimension\n",
        "assert y_hat.sum() == 1\n",
        "assert y_hat.shape == (3,)\n",
        "\n",
        "# Backwardpass\n",
        "dx = softmax_layer.backward()\n",
        "\n",
        "# Check the dimension of gradient\n",
        "assert dx.shape == (5,)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8aRB__7tpsB"
      },
      "source": [
        "## Gated Recurrent Unit layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UD0ImEp2tvMx"
      },
      "source": [
        "class GatedRecurrentUnit:\n",
        "  def __init__(self, input_size, hidden_size):\n",
        "    # Cache the input and hidden size\n",
        "    self.input_size = input_size\n",
        "    self.hidden_size = hidden_size\n",
        "\n",
        "    # Create the parameters\n",
        "    self.W_u = np.random.randn(self.hidden_size, self.hidden_size) / 1000\n",
        "    self.U_u = np.random.randn(self.hidden_size, self.input_size) / 1000\n",
        "    self.b_u = np.zeros((self.hidden_size,))\n",
        "\n",
        "    self.W_r = np.random.randn(self.hidden_size, self.hidden_size) / 1000\n",
        "    self.U_r = np.random.randn(self.hidden_size, self.input_size) / 1000\n",
        "    self.b_r = np.zeros((self.hidden_size,))\n",
        "\n",
        "    self.W_h = np.random.randn(self.hidden_size, self.hidden_size) / 1000\n",
        "    self.U_h = np.random.randn(self.hidden_size, self.input_size) / 1000\n",
        "    self.b_h = np.zeros((self.hidden_size,))\n",
        "\n",
        "  def forward(self, x):\n",
        "    # Create the first hidden state h_0\n",
        "    h_0 = np.zeros((self.hidden_size,))\n",
        "\n",
        "    # Cache the input and hidden state\n",
        "    self.x_cache = x\n",
        "    self.state_cache = [{\n",
        "        \"a_t\": None,\n",
        "        \"u_t\": None,\n",
        "        \"b_t\": None,\n",
        "        \"r_t\": None,\n",
        "        \"c_t\": None,\n",
        "        \"d_t\": None,\n",
        "        \"h_t_candidate\": None,\n",
        "        \"h_t\": h_0\n",
        "    }]\n",
        "\n",
        "    # Perform forward propagation through time\n",
        "    h = h_0\n",
        "    for t in range(len(self.x_cache)):\n",
        "      # Perform calculation\n",
        "      a_t = self.W_u @ h + self.U_u @ x[t] + self.b_u\n",
        "      u_t = sigmoid(a_t)\n",
        "\n",
        "      b_t = self.W_r @ h + self.U_r @ x[t] + self.b_r\n",
        "      r_t = sigmoid(b_t)\n",
        "\n",
        "      c_t = r_t * h\n",
        "      d_t = self.W_h @ c_t + self.U_h @ x[t] + self.b_h\n",
        "      h_t_candidate = tanh(d_t)\n",
        "\n",
        "      h_t = (1 - u_t) * h + u_t * h_t_candidate\n",
        "\n",
        "      # Cache h value\n",
        "      h = h_t\n",
        "      self.state_cache.append({\n",
        "          \"a_t\": a_t,\n",
        "          \"u_t\": u_t,\n",
        "          \"b_t\": b_t,\n",
        "          \"r_t\": r_t,\n",
        "          \"c_t\": c_t,\n",
        "          \"d_t\": d_t,\n",
        "          \"h_t_candidate\": h_t_candidate,\n",
        "          \"h_t\": h_t\n",
        "      })\n",
        "\n",
        "    # Return the final hidden state (h_T)\n",
        "    return h\n",
        "\n",
        "  def backward(self, dh_T, learning_rate=0.01):\n",
        "    # Save the gradient with respect to the weights and matrices\n",
        "    dW_u = np.zeros_like(self.W_u)\n",
        "    dU_u = np.zeros_like(self.U_u)\n",
        "    db_u = np.zeros_like(self.b_u)\n",
        "\n",
        "    dW_r = np.zeros_like(self.W_r)\n",
        "    dU_r = np.zeros_like(self.U_r)\n",
        "    db_r = np.zeros_like(self.b_r)\n",
        "\n",
        "    dW_h = np.zeros_like(self.W_h)\n",
        "    dU_h = np.zeros_like(self.U_h)\n",
        "    db_h = np.zeros_like(self.b_h)\n",
        "\n",
        "    # Start backpropagation through time\n",
        "    dh = dh_T\n",
        "\n",
        "    for t in reversed(range(len(self.x_cache))):\n",
        "      # Get input\n",
        "      x_t = self.x_cache[t]\n",
        "\n",
        "      # Unwrap the previous state\n",
        "      state_cache = self.state_cache[t+1]\n",
        "      a_t = state_cache[\"a_t\"]\n",
        "      u_t = state_cache[\"u_t\"]\n",
        "      b_t = state_cache[\"b_t\"]\n",
        "      r_t = state_cache[\"r_t\"]\n",
        "      c_t = state_cache[\"c_t\"]\n",
        "      d_t = state_cache[\"d_t\"]\n",
        "      h_t_candidate = state_cache[\"h_t_candidate\"]\n",
        "\n",
        "      # This is h_(t-1)\n",
        "      prev_cache = self.state_cache[t]\n",
        "      prev_h_t = prev_cache[\"h_t\"]\n",
        "\n",
        "      # Calculate the gradient\n",
        "      du = dh * (h_t_candidate - prev_h_t)\n",
        "      dh_candidate = dh * u_t\n",
        "      dh_prev_1 = - dh * u_t\n",
        "\n",
        "      dd = dh_candidate * (1 - tanh(d_t) ** 2)\n",
        "      dW_h_current = np.outer(dd, c_t)\n",
        "      dU_h_current = np.outer(dd, x_t)\n",
        "      db_h_current = dd\n",
        "      dc = dd @ self.W_h\n",
        "      dr = dc * prev_h_t\n",
        "      dh_prev_2 = dc * r_t\n",
        "\n",
        "      db = dr * r_t * (1 - r_t)\n",
        "      dW_r_current = np.outer(db, prev_h_t)\n",
        "      dU_r_current = np.outer(db, x_t)\n",
        "      db_r_current = db\n",
        "      dh_prev_3 = db @ self.W_r\n",
        "\n",
        "      da = du * u_t * (1 - u_t)\n",
        "      dW_u_current = np.outer(da, prev_h_t)\n",
        "      dU_u_current = np.outer(da, x_t)\n",
        "      db_u_current = da\n",
        "      dh_prev_4 = da @ self.W_u\n",
        "\n",
        "      # Accumulate to get previous h gradient\n",
        "      dh_prev = dh_prev_1 + dh_prev_2 + dh_prev_3 + dh_prev_4\n",
        "\n",
        "      # Update \n",
        "      dW_u += dW_u_current\n",
        "      dU_u += dU_u_current\n",
        "      db_u += db_u_current\n",
        "\n",
        "      dW_r += dW_r_current\n",
        "      dU_r += dU_r_current\n",
        "      db_r += db_r_current\n",
        "\n",
        "      dW_h += dW_h_current\n",
        "      dU_h += dU_h_current\n",
        "      db_h += db_h_current\n",
        "\n",
        "      # Assign new dh\n",
        "      dh = dh_prev\n",
        "\n",
        "    # Gradient clipping to prevent\n",
        "    # gradient explosion\n",
        "    dW_u = np.clip(dW_u, -1, 1)\n",
        "    dU_u = np.clip(dU_u, -1, 1)\n",
        "    db_u = np.clip(db_u, -1, 1)\n",
        "\n",
        "    dW_r = np.clip(dW_r, -1, 1)\n",
        "    dU_r = np.clip(dU_r, -1, 1)\n",
        "    db_r = np.clip(db_r, -1, 1)\n",
        "\n",
        "    dW_h = np.clip(dW_h, -1, 1)\n",
        "    dU_h = np.clip(dU_h, -1, 1)\n",
        "    db_h = np.clip(db_h, -1, 1)\n",
        "\n",
        "    # Update the parameters\n",
        "    self.W_u -= learning_rate * dW_u\n",
        "    self.U_u -= learning_rate * dU_u\n",
        "    self.b_u -= learning_rate * db_u\n",
        "\n",
        "    self.W_r -= learning_rate * dW_r\n",
        "    self.U_r -= learning_rate * dU_r\n",
        "    self.b_r -= learning_rate * db_r\n",
        "\n",
        "    self.W_h -= learning_rate * dW_h\n",
        "    self.U_h -= learning_rate * dU_h\n",
        "    self.b_h -= learning_rate * db_h"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "djtqbdtVytv-"
      },
      "source": [
        "# Test gru layer with training example\n",
        "input, target = train_dataset[5]\n",
        "\n",
        "# Define the size\n",
        "input_size = input[0].shape[0]\n",
        "hidden_size = 64\n",
        "\n",
        "# Create layer\n",
        "gru = GatedRecurrentUnit(\n",
        "    input_size=input_size,\n",
        "    hidden_size=64\n",
        ")\n",
        "\n",
        "# Forward pass\n",
        "final_hidden_state = gru.forward(input)\n",
        "\n",
        "# Check dimension\n",
        "assert final_hidden_state.shape == (64,)\n",
        "assert len(gru.x_cache) == len(input)\n",
        "assert len(gru.state_cache) == len(input) + 1\n",
        "\n",
        "# Check backward pass\n",
        "dh_T = np.random.randn(64)\n",
        "gru.backward(dh_T)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWZqhuFP6KvA"
      },
      "source": [
        "## Training phase"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oABICMxtzgwt"
      },
      "source": [
        "def train(rnn, log, epochs = 2000):\n",
        "  for i in range(epochs):\n",
        "    loss = 0\n",
        "    train_correct = 0\n",
        "    test_correct = 0\n",
        "\n",
        "    # Loop through items in train dataset and train\n",
        "    for x, y in train_dataset:\n",
        "      h_T = rnn.forward(x)\n",
        "      y_hat = log.forward(h_T)\n",
        "\n",
        "      current_loss = - y * np.log(y_hat) - (1 - y) * np.log(1 - y_hat)\n",
        "      loss += current_loss\n",
        "\n",
        "      if (y == 0 and y_hat < 0.5) or (y == 1 and y_hat >= 0.5):\n",
        "        train_correct += 1\n",
        "\n",
        "      dh_T = log.backward(y)\n",
        "      rnn.backward(dh_T)\n",
        "\n",
        "    # Calculate test accuracy\n",
        "    for x, y in test_dataset:\n",
        "      h_T = rnn.forward(x)\n",
        "      y_hat = log.forward(h_T)\n",
        "\n",
        "      if (y == 0 and y_hat < 0.5) or (y == 1 and y_hat >= 0.5):\n",
        "        test_correct += 1\n",
        "\n",
        "    print(f\"Epochs: {i}\")\n",
        "    print(f\"Loss: {loss / len(train_dataset)}\")\n",
        "    print(f\"Train Accuracy: {train_correct / len(train_dataset)}\")\n",
        "    print(f\"Test Accuracy: {test_correct / len(test_dataset)}\")\n",
        "    print(\"--------------------\")"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "cPFp2k4N65sw",
        "outputId": "f65a5ec3-6394-4d0f-a68c-f5bfd989cbd9"
      },
      "source": [
        "# Create GRU and Softmax\n",
        "gru_layer = GatedRecurrentUnit(\n",
        "    input_size=input_size,\n",
        "    hidden_size=64\n",
        ")\n",
        "\n",
        "log_layer = LogisticRegression(\n",
        "    input_size=64,\n",
        "    learning_rate=0.01\n",
        ")\n",
        "\n",
        "# Train\n",
        "train(gru_layer, log_layer)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 279\n",
            "Loss: 0.014942297125941953\n",
            "Train Accuracy: 1.0\n",
            "Test Accuracy: 1.0\n",
            "--------------------\n",
            "Epochs: 280\n",
            "Loss: 0.014782132824850214\n",
            "Train Accuracy: 1.0\n",
            "Test Accuracy: 1.0\n",
            "--------------------\n",
            "Epochs: 281\n",
            "Loss: 0.014625460827046787\n",
            "Train Accuracy: 1.0\n",
            "Test Accuracy: 1.0\n",
            "--------------------\n",
            "Epochs: 282\n",
            "Loss: 0.014472177426328425\n",
            "Train Accuracy: 1.0\n",
            "Test Accuracy: 1.0\n",
            "--------------------\n",
            "Epochs: 283\n",
            "Loss: 0.014322173591104102\n",
            "Train Accuracy: 1.0\n",
            "Test Accuracy: 1.0\n",
            "--------------------\n",
            "Epochs: 284\n",
            "Loss: 0.014175343084418687\n",
            "Train Accuracy: 1.0\n",
            "Test Accuracy: 1.0\n",
            "--------------------\n",
            "Epochs: 285\n",
            "Loss: 0.014031559943360233\n",
            "Train Accuracy: 1.0\n",
            "Test Accuracy: 1.0\n",
            "--------------------\n",
            "Epochs: 286\n",
            "Loss: 0.013890628906226929\n",
            "Train Accuracy: 1.0\n",
            "Test Accuracy: 1.0\n",
            "--------------------\n",
            "Epochs: 287\n",
            "Loss: 0.013752715517049185\n",
            "Train Accuracy: 1.0\n",
            "Test Accuracy: 1.0\n",
            "--------------------\n",
            "Epochs: 288\n",
            "Loss: 0.013617740621092923\n",
            "Train Accuracy: 1.0\n",
            "Test Accuracy: 1.0\n",
            "--------------------\n",
            "Epochs: 289\n",
            "Loss: 0.013485601836947463\n",
            "Train Accuracy: 1.0\n",
            "Test Accuracy: 1.0\n",
            "--------------------\n",
            "Epochs: 290\n",
            "Loss: 0.013356200131263743\n",
            "Train Accuracy: 1.0\n",
            "Test Accuracy: 1.0\n",
            "--------------------\n",
            "Epochs: 291\n",
            "Loss: 0.013229441499901476\n",
            "Train Accuracy: 1.0\n",
            "Test Accuracy: 1.0\n",
            "--------------------\n",
            "Epochs: 292\n",
            "Loss: 0.013105236432533796\n",
            "Train Accuracy: 1.0\n",
            "Test Accuracy: 1.0\n",
            "--------------------\n",
            "Epochs: 293\n",
            "Loss: 0.01298349952947314\n",
            "Train Accuracy: 1.0\n",
            "Test Accuracy: 1.0\n",
            "--------------------\n",
            "Epochs: 294\n",
            "Loss: 0.012864149276410311\n",
            "Train Accuracy: 1.0\n",
            "Test Accuracy: 1.0\n",
            "--------------------\n",
            "Epochs: 295\n",
            "Loss: 0.012747061179545549\n",
            "Train Accuracy: 1.0\n",
            "Test Accuracy: 1.0\n",
            "--------------------\n",
            "Epochs: 296\n",
            "Loss: 0.012632225035164556\n",
            "Train Accuracy: 1.0\n",
            "Test Accuracy: 1.0\n",
            "--------------------\n",
            "Epochs: 297\n",
            "Loss: 0.012519693624563137\n",
            "Train Accuracy: 1.0\n",
            "Test Accuracy: 1.0\n",
            "--------------------\n",
            "Epochs: 298\n",
            "Loss: 0.012409304289347759\n",
            "Train Accuracy: 1.0\n",
            "Test Accuracy: 1.0\n",
            "--------------------\n",
            "Epochs: 299\n",
            "Loss: 0.012300977230269431\n",
            "Train Accuracy: 1.0\n",
            "Test Accuracy: 1.0\n",
            "--------------------\n",
            "Epochs: 300\n",
            "Loss: 0.01219464662049914\n",
            "Train Accuracy: 1.0\n",
            "Test Accuracy: 1.0\n",
            "--------------------\n",
            "Epochs: 301\n",
            "Loss: 0.01209025020850352\n",
            "Train Accuracy: 1.0\n",
            "Test Accuracy: 1.0\n",
            "--------------------\n",
            "Epochs: 302\n",
            "Loss: 0.011987728335923698\n",
            "Train Accuracy: 1.0\n",
            "Test Accuracy: 1.0\n",
            "--------------------\n",
            "Epochs: 303\n",
            "Loss: 0.011887024000654353\n",
            "Train Accuracy: 1.0\n",
            "Test Accuracy: 1.0\n",
            "--------------------\n",
            "Epochs: 304\n",
            "Loss: 0.011788082840534581\n",
            "Train Accuracy: 1.0\n",
            "Test Accuracy: 1.0\n",
            "--------------------\n",
            "Epochs: 305\n",
            "Loss: 0.011690839432369378\n",
            "Train Accuracy: 1.0\n",
            "Test Accuracy: 1.0\n",
            "--------------------\n",
            "Epochs: 306\n",
            "Loss: 0.011595069322963055\n",
            "Train Accuracy: 1.0\n",
            "Test Accuracy: 1.0\n",
            "--------------------\n",
            "Epochs: 307\n",
            "Loss: 0.01150061385218636\n",
            "Train Accuracy: 1.0\n",
            "Test Accuracy: 1.0\n",
            "--------------------\n",
            "Epochs: 308\n",
            "Loss: 0.01140758549721661\n",
            "Train Accuracy: 1.0\n",
            "Test Accuracy: 1.0\n",
            "--------------------\n",
            "Epochs: 309\n",
            "Loss: 0.0113161507979811\n",
            "Train Accuracy: 1.0\n",
            "Test Accuracy: 1.0\n",
            "--------------------\n",
            "Epochs: 310\n",
            "Loss: 0.011226201543310449\n",
            "Train Accuracy: 1.0\n",
            "Test Accuracy: 1.0\n",
            "--------------------\n",
            "Epochs: 311\n",
            "Loss: 0.011137681698395784\n",
            "Train Accuracy: 1.0\n",
            "Test Accuracy: 1.0\n",
            "--------------------\n",
            "Epochs: 312\n",
            "Loss: 0.011050555598915157\n",
            "Train Accuracy: 1.0\n",
            "Test Accuracy: 1.0\n",
            "--------------------\n",
            "Epochs: 313\n",
            "Loss: 0.01096479060637567\n",
            "Train Accuracy: 1.0\n",
            "Test Accuracy: 1.0\n",
            "--------------------\n",
            "Epochs: 314\n",
            "Loss: 0.010880354688841819\n",
            "Train Accuracy: 1.0\n",
            "Test Accuracy: 1.0\n",
            "--------------------\n",
            "Epochs: 315\n",
            "Loss: 0.01079721639826523\n",
            "Train Accuracy: 1.0\n",
            "Test Accuracy: 1.0\n",
            "--------------------\n",
            "Epochs: 316\n",
            "Loss: 0.010715345050483572\n",
            "Train Accuracy: 1.0\n",
            "Test Accuracy: 1.0\n",
            "--------------------\n",
            "Epochs: 317\n",
            "Loss: 0.010634710824667585\n",
            "Train Accuracy: 1.0\n",
            "Test Accuracy: 1.0\n",
            "--------------------\n",
            "Epochs: 318\n",
            "Loss: 0.010555284795497135\n",
            "Train Accuracy: 1.0\n",
            "Test Accuracy: 1.0\n",
            "--------------------\n",
            "Epochs: 319\n",
            "Loss: 0.01047703893056725\n",
            "Train Accuracy: 1.0\n",
            "Test Accuracy: 1.0\n",
            "--------------------\n",
            "Epochs: 320\n",
            "Loss: 0.01039994607199908\n",
            "Train Accuracy: 1.0\n",
            "Test Accuracy: 1.0\n",
            "--------------------\n",
            "Epochs: 321\n",
            "Loss: 0.010323962149182795\n",
            "Train Accuracy: 1.0\n",
            "Test Accuracy: 1.0\n",
            "--------------------\n",
            "Epochs: 322\n",
            "Loss: 0.010248934357454668\n",
            "Train Accuracy: 1.0\n",
            "Test Accuracy: 1.0\n",
            "--------------------\n",
            "Epochs: 323\n",
            "Loss: 0.010174833483430111\n",
            "Train Accuracy: 1.0\n",
            "Test Accuracy: 1.0\n",
            "--------------------\n",
            "Epochs: 324\n",
            "Loss: 0.010101767435324089\n",
            "Train Accuracy: 1.0\n",
            "Test Accuracy: 1.0\n",
            "--------------------\n",
            "Epochs: 325\n",
            "Loss: 0.01002974025726496\n",
            "Train Accuracy: 1.0\n",
            "Test Accuracy: 1.0\n",
            "--------------------\n",
            "Epochs: 326\n",
            "Loss: 0.009958733941562391\n",
            "Train Accuracy: 1.0\n",
            "Test Accuracy: 1.0\n",
            "--------------------\n",
            "Epochs: 327\n",
            "Loss: 0.009888726658766459\n",
            "Train Accuracy: 1.0\n",
            "Test Accuracy: 1.0\n",
            "--------------------\n",
            "Epochs: 328\n",
            "Loss: 0.009819696543027182\n",
            "Train Accuracy: 1.0\n",
            "Test Accuracy: 1.0\n",
            "--------------------\n",
            "Epochs: 329\n",
            "Loss: 0.009751622336987072\n",
            "Train Accuracy: 1.0\n",
            "Test Accuracy: 1.0\n",
            "--------------------\n",
            "Epochs: 330\n",
            "Loss: 0.009684483426821038\n",
            "Train Accuracy: 1.0\n",
            "Test Accuracy: 1.0\n",
            "--------------------\n",
            "Epochs: 331\n",
            "Loss: 0.009618259795081041\n",
            "Train Accuracy: 1.0\n",
            "Test Accuracy: 1.0\n",
            "--------------------\n",
            "Epochs: 332\n",
            "Loss: 0.009552931981823594\n",
            "Train Accuracy: 1.0\n",
            "Test Accuracy: 1.0\n",
            "--------------------\n",
            "Epochs: 333\n",
            "Loss: 0.009488481059478172\n",
            "Train Accuracy: 1.0\n",
            "Test Accuracy: 1.0\n",
            "--------------------\n",
            "Epochs: 334\n",
            "Loss: 0.009424888615630674\n",
            "Train Accuracy: 1.0\n",
            "Test Accuracy: 1.0\n",
            "--------------------\n",
            "Epochs: 335\n",
            "Loss: 0.00936213673929776\n",
            "Train Accuracy: 1.0\n",
            "Test Accuracy: 1.0\n",
            "--------------------\n",
            "Epochs: 336\n",
            "Loss: 0.00930020800841349\n",
            "Train Accuracy: 1.0\n",
            "Test Accuracy: 1.0\n",
            "--------------------\n",
            "Epochs: 337\n",
            "Loss: 0.009239085477546408\n",
            "Train Accuracy: 1.0\n",
            "Test Accuracy: 1.0\n",
            "--------------------\n",
            "Epochs: 338\n",
            "Loss: 0.009178752665499983\n",
            "Train Accuracy: 1.0\n",
            "Test Accuracy: 1.0\n",
            "--------------------\n",
            "Epochs: 339\n",
            "Loss: 0.009119193542731503\n",
            "Train Accuracy: 1.0\n",
            "Test Accuracy: 1.0\n",
            "--------------------\n",
            "Epochs: 340\n",
            "Loss: 0.00906039251863372\n",
            "Train Accuracy: 1.0\n",
            "Test Accuracy: 1.0\n",
            "--------------------\n",
            "Epochs: 341\n",
            "Loss: 0.00900233442876005\n",
            "Train Accuracy: 1.0\n",
            "Test Accuracy: 1.0\n",
            "--------------------\n",
            "Epochs: 342\n",
            "Loss: 0.008945004522078028\n",
            "Train Accuracy: 1.0\n",
            "Test Accuracy: 1.0\n",
            "--------------------\n",
            "Epochs: 343\n",
            "Loss: 0.008888388448326602\n",
            "Train Accuracy: 1.0\n",
            "Test Accuracy: 1.0\n",
            "--------------------\n",
            "Epochs: 344\n",
            "Loss: 0.00883247224554155\n",
            "Train Accuracy: 1.0\n",
            "Test Accuracy: 1.0\n",
            "--------------------\n",
            "Epochs: 345\n",
            "Loss: 0.008777242327800638\n",
            "Train Accuracy: 1.0\n",
            "Test Accuracy: 1.0\n",
            "--------------------\n",
            "Epochs: 346\n",
            "Loss: 0.008722685473228471\n",
            "Train Accuracy: 1.0\n",
            "Test Accuracy: 1.0\n",
            "--------------------\n",
            "Epochs: 347\n",
            "Loss: 0.008668788812292376\n",
            "Train Accuracy: 1.0\n",
            "Test Accuracy: 1.0\n",
            "--------------------\n",
            "Epochs: 348\n",
            "Loss: 0.00861553981641103\n",
            "Train Accuracy: 1.0\n",
            "Test Accuracy: 1.0\n",
            "--------------------\n",
            "Epochs: 349\n",
            "Loss: 0.008562926286891263\n",
            "Train Accuracy: 1.0\n",
            "Test Accuracy: 1.0\n",
            "--------------------\n",
            "Epochs: 350\n",
            "Loss: 0.008510936344203133\n",
            "Train Accuracy: 1.0\n",
            "Test Accuracy: 1.0\n",
            "--------------------\n",
            "Epochs: 351\n",
            "Loss: 0.008459558417598071\n",
            "Train Accuracy: 1.0\n",
            "Test Accuracy: 1.0\n",
            "--------------------\n",
            "Epochs: 352\n",
            "Loss: 0.00840878123507158\n",
            "Train Accuracy: 1.0\n",
            "Test Accuracy: 1.0\n",
            "--------------------\n",
            "Epochs: 353\n",
            "Loss: 0.008358593813668568\n",
            "Train Accuracy: 1.0\n",
            "Test Accuracy: 1.0\n",
            "--------------------\n",
            "Epochs: 354\n",
            "Loss: 0.008308985450127844\n",
            "Train Accuracy: 1.0\n",
            "Test Accuracy: 1.0\n",
            "--------------------\n",
            "Epochs: 355\n",
            "Loss: 0.008259945711859158\n",
            "Train Accuracy: 1.0\n",
            "Test Accuracy: 1.0\n",
            "--------------------\n",
            "Epochs: 356\n",
            "Loss: 0.008211464428246171\n",
            "Train Accuracy: 1.0\n",
            "Test Accuracy: 1.0\n",
            "--------------------\n",
            "Epochs: 357\n",
            "Loss: 0.008163531682266705\n",
            "Train Accuracy: 1.0\n",
            "Test Accuracy: 1.0\n",
            "--------------------\n",
            "Epochs: 358\n",
            "Loss: 0.008116137802420794\n",
            "Train Accuracy: 1.0\n",
            "Test Accuracy: 1.0\n",
            "--------------------\n",
            "Epochs: 359\n",
            "Loss: 0.0080692746990568\n",
            "Train Accuracy: 1.0\n",
            "Test Accuracy: 1.0\n",
            "--------------------\n",
            "Epochs: 360\n",
            "Loss: 0.008022937604888508\n",
            "Train Accuracy: 1.0\n",
            "Test Accuracy: 1.0\n",
            "--------------------\n",
            "Epochs: 361\n",
            "Loss: 0.007977100668836582\n",
            "Train Accuracy: 1.0\n",
            "Test Accuracy: 1.0\n",
            "--------------------\n",
            "Epochs: 362\n",
            "Loss: 0.007931765664106737\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-59a67a32a008>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgru_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-24-b70b17e0e4d3>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(rnn, log, epochs)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epochs: {i}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Loss: {loss / len(train_dataset)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Train Accuracy: {train_correct / len(train_dataset)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Test Accuracy: {test_correct / len(test_dataset)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/iostream.py\u001b[0m in \u001b[0;36mwrite\u001b[0;34m(self, string)\u001b[0m\n\u001b[1;32m    398\u001b[0m             \u001b[0mis_child\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_master_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m             \u001b[0;31m# only touch the buffer in the IO thread to avoid races\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 400\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpub_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschedule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    401\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_child\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m                 \u001b[0;31m# newlines imply flush in subprocesses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/iostream.py\u001b[0m in \u001b[0;36mschedule\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m    201\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_events\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m             \u001b[0;31m# wake event thread (message content is ignored)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event_pipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mb''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m             \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, data, flags, copy, track, routing_id, group)\u001b[0m\n\u001b[1;32m    545\u001b[0m                 )\n\u001b[1;32m    546\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSocket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msend_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg_parts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.send\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.send\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._send_copy\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WjabqDCO7LSI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}